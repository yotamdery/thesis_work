{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import sys\n",
    "sys.path.append('../SEIR_full/')\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "import itertools\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.sparse import csr_matrix\n",
    "import sys\n",
    "import os\n",
    "from SEIR_full.indices import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Generating parameters files based on tazs #\n",
    "#############################################\n",
    "\n",
    "cell_name = '250'\n",
    "isr_pop = 9136000\n",
    "\n",
    "# add functions\n",
    "def make_pop(df):\n",
    "    df = df.iloc[:, 0:-2]\n",
    "    return df.sum(axis=0)\n",
    "\n",
    "\n",
    "def make_pop_religion(df):\n",
    "    df = df.iloc[:, 1:8].multiply(df['tot_pop'], axis='index')\n",
    "    return df.sum(axis=0)\n",
    "\n",
    "\n",
    "def robust_max(srs, n=3):\n",
    "    sort = sorted(srs)\n",
    "    return np.mean(sort[-n:])\n",
    "\n",
    "\n",
    "def robust_min(srs, n=3):\n",
    "    sort = sorted(srs)\n",
    "    return np.mean(sort[:n])\n",
    "\n",
    "\n",
    "def weighted_std(values, weights):\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values - average) ** 2, weights=weights)\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "\n",
    "def avg_by_dates(df, from_date, to_date, weights=None):\n",
    "    filtered = df[(df.index >= from_date) & (df.index <= to_date)]\n",
    "    if weights is None:\n",
    "        return filtered.describe().T[['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    weights = pd.Series(weights)\n",
    "    stats = filtered.describe().T[['min', 'max']]\n",
    "    stats['mean'] = filtered.apply(\n",
    "        lambda col: np.average(col, weights=weights))\n",
    "    stats['std'] = filtered.apply(\n",
    "        lambda col: weighted_std(col, weights=weights))\n",
    "    return stats\n",
    "\n",
    "\n",
    "def wheighted_average(df):\n",
    "    tot = df['tot_pop'].sum()\n",
    "    return (df['cases_prop'].sum() / tot)\n",
    "\n",
    "def normelize(row, global_min, span):\n",
    "    new_row = (row - global_min) / span\n",
    "    new_row = np.minimum(new_row, 1.0)\n",
    "    new_row = np.maximum(new_row, 1e-6)\n",
    "    return new_row\n",
    "\n",
    "\n",
    "def create_demograph_age_dist_empty_cells(ind):\n",
    "    ### Creating demograph/age_dist\n",
    "    pop_dist = pd.read_excel('../Data/raw/pop2taz.xlsx', header=2)\n",
    "    ages_list = ['Unnamed: ' + str(i) for i in range(17, 32)]\n",
    "    pop_dist = pop_dist[['אזור 2630', 'גילאים'] + ages_list]\n",
    "    pop_dist.columns = ['id'] + list(pop_dist.iloc[0, 1:])\n",
    "    pop_dist = pop_dist.drop([0, 2631, 2632, 2633])\n",
    "    pop_dist['tot_pop'] = pop_dist.iloc[:, 1:].sum(axis=1)\n",
    "    pop_dist['tot_pop'].loc[pop_dist['tot_pop'] == 0] = 1\n",
    "    pop_dist = pop_dist.iloc[:, 1:-1].div(pop_dist['tot_pop'], axis=0).join(\n",
    "        pop_dist['id']).join(pop_dist['tot_pop'])\n",
    "    pop_dist['tot_pop'].loc[pop_dist['tot_pop'] == 1] = 0\n",
    "    pop_dist['tot_pop'] = pop_dist['tot_pop'] / pop_dist['tot_pop'].sum()\n",
    "    pop_dist.iloc[:, :-2] = pop_dist.iloc[:, :-2].mul(pop_dist['tot_pop'],\n",
    "                                                      axis=0)\n",
    "\n",
    "    taz2cell = pd.read_excel(\n",
    "        '../Data/division_choice/' + ind.cell_name + '/taz2cell.xlsx')\n",
    "    taz2cell = taz2cell[['taz_id', 'cell_id']]\n",
    "    taz2cell.columns = ['id', 'new_id']\n",
    "\n",
    "    pop_cell = pop_dist.merge(taz2cell, left_on='id', right_on='id')\n",
    "    pop_cell['new_id'] = pop_cell['new_id'].astype(str)\n",
    "    pop_cell.sort_values(by='new_id')\n",
    "\n",
    "    pop_cell = pop_cell.groupby(by='new_id').apply(lambda df: make_pop(df))\n",
    "    pop_cell['10-19'] = pop_cell['10-14'] + pop_cell['15-19']\n",
    "    pop_cell['20-29'] = pop_cell['20-24'] + pop_cell['25-29']\n",
    "    pop_cell['30-39'] = pop_cell['30-34'] + pop_cell['35-39']\n",
    "    pop_cell['40-49'] = pop_cell['40-44'] + pop_cell['45-49']\n",
    "    pop_cell['50-59'] = pop_cell['50-54'] + pop_cell['55-59']\n",
    "    pop_cell['60-69'] = pop_cell['60-64'] + pop_cell['65-69']\n",
    "    pop_cell['70+'] = pop_cell['70-74'] + pop_cell['75+']\n",
    "    pop_cell = pop_cell[list(ind.A.values())]\n",
    "    pop_cell = pop_cell / pop_cell.sum().sum()\n",
    "    pop_cell.reset_index(inplace=True)\n",
    "    pop_cell.columns = ['cell_id'] + list(ind.A.values())\n",
    "\n",
    "    ## empty cells file to save\n",
    "    try:\n",
    "        os.mkdir('../Data/demograph')\n",
    "    except:\n",
    "        pass\n",
    "    empty_cells = pop_cell[pop_cell.sum(axis=1) == 0]['cell_id']\n",
    "    empty_cells.to_csv('../Data/demograph/empty_cells.csv')\n",
    "\n",
    "    empty_cells = pd.read_csv('../Data/demograph/empty_cells.csv')[\n",
    "        'cell_id'].astype(str)\n",
    "    pop_cell = pop_cell[\n",
    "        pop_cell['cell_id'].apply(lambda x: x not in empty_cells.values)]\n",
    "    pop_cell.to_csv('../Data/demograph/age_dist_area.csv')\n",
    "\n",
    "\n",
    "def create_paramaters_ind(ind):\n",
    "    ind.update_empty()\n",
    "    ## empty cells file to save\n",
    "    try:\n",
    "        os.mkdir('../Data/parameters')\n",
    "    except:\n",
    "        pass\n",
    "    with open('../Data/parameters/indices.pickle', 'wb') as handle:\n",
    "        pickle.dump(ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def create_demograph_religion(ind):\n",
    "    ### creating demograph/religion\n",
    "    religion2taz = pd.read_csv('../Data/raw/religion2taz.csv')\n",
    "    religion2taz.sort_values(by='taz_id', inplace=True)\n",
    "    religion2taz.columns = ['id', 'Orthodox', 'Druze', 'Other', 'Sacular',\n",
    "                            'Muslim', 'Christian']\n",
    "    religion2taz['Jewish'] = religion2taz['Orthodox'] + religion2taz['Sacular']\n",
    "    taz2cell = pd.read_excel(\n",
    "        '../Data/division_choice/' + ind.cell_name + '/taz2cell.xlsx')\n",
    "    taz2cell = taz2cell[['taz_id', 'cell_id']]\n",
    "    taz2cell.columns = ['id', 'new_id']\n",
    "    religion2taz = religion2taz.merge(taz2cell, on='id')\n",
    "    religion2taz['new_id'] = religion2taz['new_id'].astype(str)\n",
    "    religion2taz.sort_values(by='new_id', inplace=True)\n",
    "    pop_dist = pd.read_excel('../Data/raw/pop2taz.xlsx', header=2)\n",
    "    ages_list = ['Unnamed: ' + str(i) for i in range(17, 32)]\n",
    "\n",
    "    pop_dist = pop_dist[['אזור 2630', 'גילאים'] + ages_list]\n",
    "    pop_dist.columns = ['id'] + list(pop_dist.iloc[0, 1:])\n",
    "    pop_dist = pop_dist.drop([0, 2631, 2632, 2633])\n",
    "    pop_dist['tot_pop'] = pop_dist.iloc[:, 1:].sum(axis=1)\n",
    "    pop_dist['tot_pop'].loc[pop_dist['tot_pop'] == 0] = 1\n",
    "    pop_dist = pop_dist.iloc[:, 1:-1].div(pop_dist['tot_pop'], axis=0).join(\n",
    "        pop_dist['id']).join(pop_dist['tot_pop'])\n",
    "    pop_dist['tot_pop'].loc[pop_dist['tot_pop'] == 1] = 0\n",
    "    pop_dist['tot_pop'] = pop_dist['tot_pop'] / pop_dist['tot_pop'].sum()\n",
    "    pop_dist.iloc[:, :-2] = pop_dist.iloc[:, :-2].mul(pop_dist['tot_pop'],\n",
    "                                                      axis=0)\n",
    "    pop_dist = pop_dist[['id', 'tot_pop']]\n",
    "\n",
    "    religion2taz = religion2taz.merge(pop_dist, on='id')\n",
    "    religion2taz.sort_values(by='id', inplace=True)\n",
    "\n",
    "    # fixing religion city factor\n",
    "    if ind.cell_name == '20':\n",
    "        cell_num = len(list(set(religion2taz['new_id'])))\n",
    "        factor = pd.DataFrame({'new_id': list(set(religion2taz['new_id'])),\n",
    "                               'orth_factor': [1] * cell_num,\n",
    "                               'arab_factor': [1] * cell_num, }).sort_values(\n",
    "            by='new_id')\n",
    "        factor = factor.reset_index().drop(['index'], axis=1)\n",
    "        factor.iloc[:, 1] = pd.Series(\n",
    "            [1,\n",
    "             0.48 / 0.41,\n",
    "             0.13 / 0.04,\n",
    "             0.05 / 0.02,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             0.1 / 0.05,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             0.82 / 0.6,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             0.24 / 0.36])\n",
    "        factor.iloc[:, 2] = pd.Series(\n",
    "            [0.38 / 0.01,\n",
    "             1,\n",
    "             0.106 / 0.01,\n",
    "             0.36 / 0.14,\n",
    "             0.65 / 0.4,\n",
    "             1.1 / 0.38,\n",
    "             1.1 / 0.1,\n",
    "             0.15 / 0.07,\n",
    "             0.6 / 0.3,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1,\n",
    "             1.3 / 0.8,\n",
    "             1])\n",
    "\n",
    "        religion2taz = religion2taz.merge(factor, on='new_id')\n",
    "        religion2taz['Orthodox'] = religion2taz['Orthodox'] * religion2taz[\n",
    "            'orth_factor']\n",
    "        religion2taz['Sacular'] = religion2taz['Sacular'] - religion2taz[\n",
    "            'Orthodox'] * (religion2taz['orth_factor'] - 1)\n",
    "        religion2taz['Muslim'] = religion2taz['Muslim'] * religion2taz[\n",
    "            'arab_factor']\n",
    "\n",
    "    religion2taz = religion2taz.groupby(by='new_id').apply(make_pop_religion)\n",
    "    tmp = religion2taz[\n",
    "        ['Druze', 'Other', 'Muslim', 'Christian', 'Jewish']].sum(axis=1)\n",
    "    tmp.loc[tmp == 0] = 1\n",
    "    religion2taz = religion2taz.divide(tmp, axis=0)\n",
    "    religion2taz.reset_index(inplace=True)\n",
    "    religion2taz.columns = ['cell_id', 'Orthodox', 'Druze', 'Other', 'Sacular',\n",
    "                            'Muslim', 'Christian', 'Jewish']\n",
    "    religion2taz['cell_id'] = religion2taz['cell_id'].astype(str)\n",
    "    empty_cells = pd.read_csv('../Data/demograph/empty_cells.csv')[\n",
    "        'cell_id'].astype(str)\n",
    "    religion2taz = religion2taz[\n",
    "        religion2taz['cell_id'].apply(lambda x: x not in empty_cells.values)]\n",
    "    religion2taz.to_csv('../Data/demograph/religion_dis.csv')\n",
    "\n",
    "\n",
    "def create_stay_home(ind):\n",
    "    ## Creating stay_home/ALL\n",
    "    home = pd.read_csv('../Data/raw/Summary_Home_0_TAZ.csv')\n",
    "    home = home.iloc[:, 1:]\n",
    "    home.columns = ['date', 'taz_id', 'stay', 'out']\n",
    "    home['date'] = pd.to_datetime(home['date'], dayfirst=True)\n",
    "    home['stay'] = home['stay'].apply(lambda x: x.replace(',', '')).astype(int)\n",
    "    home['out'] = home['out'].apply(lambda x: x.replace(',', '')).astype(int)\n",
    "    home['total'] = home['stay'] + home['out']\n",
    "    home['out_pct'] = home['out'] / home['total']\n",
    "\n",
    "    taz2cell = pd.read_excel(\n",
    "        '../Data/division_choice/' + ind.cell_name + '/taz2cell.xlsx')\n",
    "    home = home.merge(taz2cell, on='taz_id')\n",
    "    home['cell_id'] = home['cell_id'].astype(str)\n",
    "    empty_cells = pd.read_csv('../Data/demograph/empty_cells.csv')[\n",
    "        'cell_id'].astype(str)\n",
    "    home = home[\n",
    "        home['cell_id'].apply(lambda x: x not in empty_cells.values)]\n",
    "\n",
    "    home_cell = home.groupby(['date', 'cell_id'])[\n",
    "        ['stay', 'out', 'total']].sum().reset_index()\n",
    "    home_cell['out_pct'] = home_cell['out'] / home_cell['total']\n",
    "    home_cell = home_cell.set_index('date')\n",
    "    home_cell = home_cell.groupby(by='cell_id')['out_pct'].rolling(7,\n",
    "                                                                   center=True).mean()\n",
    "    home_cell = home_cell.unstack(level=0).dropna()\n",
    "\n",
    "    global_max = home_cell.apply(robust_max)\n",
    "    global_min = home_cell.apply(robust_min)\n",
    "    span = global_max - global_min\n",
    "    relative_rate = home_cell.apply(\n",
    "        lambda row: normelize(row, global_min, span),\n",
    "        axis=1)\n",
    "\n",
    "    result = dict()\n",
    "    result['routine'] = avg_by_dates(relative_rate, '2020-02-02', '2020-02-29')\n",
    "    result['no_school'] = avg_by_dates(relative_rate, '2020-03-14',\n",
    "                                       '2020-03-16',\n",
    "                                       weights={'2020-03-14': 2 / 7,\n",
    "                                                '2020-03-15': 2.5 / 7,\n",
    "                                                '2020-03-16': 2.5 / 7})\n",
    "    result['no_work'] = avg_by_dates(relative_rate, '2020-03-17', '2020-03-25',\n",
    "                                     weights={\n",
    "                                     i: 1 / 14 if i.day in [17, 18, 24,\n",
    "                                                            25] else 1 / 7\n",
    "                                     for i in pd.date_range('2020-03-17',\n",
    "                                                            '2020-03-25')})\n",
    "\n",
    "    result['no_100_meters'] = avg_by_dates(relative_rate, '2020-03-26',\n",
    "                                           '2020-04-02',\n",
    "                                           weights={i: 1 / 14 if i.day in [26,\n",
    "                                                                           2] else 1 / 7\n",
    "                                                    for i in\n",
    "                                                    pd.date_range('2020-03-26',\n",
    "                                                                  '2020-04-02')})\n",
    "\n",
    "    result['no_bb'] = avg_by_dates(\n",
    "        relative_rate,\n",
    "        '2020-04-03',\n",
    "        '2020-04-06',\n",
    "        weights={i: 5 / 14 if i.day in [5, 6] else 1 / 7\n",
    "                 for i in pd.date_range('2020-04-03', '2020-04-06')\n",
    "                 },\n",
    "    )\n",
    "\n",
    "    result['full_lockdown'] = avg_by_dates(\n",
    "        relative_rate,\n",
    "        '2020-04-07',\n",
    "        '2020-04-16',\n",
    "        weights={i: 5 / 28 if i.day in [7, 12, 13, 16] else 1 / 21\n",
    "                 for i in pd.date_range('2020-04-07', '2020-04-16')},\n",
    "    )\n",
    "\n",
    "    weights_release = dict()\n",
    "    for i in pd.date_range('2020-04-17', '2020-05-02'):\n",
    "        if i.day in [17, 18, 24, 25, 1, 2]:\n",
    "            weights_release[i] = 1 / 21\n",
    "        elif i.day in [28, 29]:\n",
    "            weights_release[i] = 0\n",
    "        else:\n",
    "            weights_release[i] = 5 / 56\n",
    "    result['release'] = avg_by_dates(\n",
    "        relative_rate,\n",
    "        '2020-04-17',\n",
    "        '2020-05-02',\n",
    "        weights=weights_release,\n",
    "    )\n",
    "    # save\n",
    "    try:\n",
    "        os.mkdir('../Data/stay_home')\n",
    "    except:\n",
    "        pass\n",
    "    result['routine'].to_csv('../Data/stay_home/routine.csv')\n",
    "    result['no_school'].to_csv('../Data/stay_home/no_school.csv')\n",
    "    result['no_work'].to_csv('../Data/stay_home/no_work.csv')\n",
    "    result['no_100_meters'].to_csv('../Data/stay_home/no_100_meters.csv')\n",
    "    result['no_bb'].to_csv('../Data/stay_home/no_bb.csv')\n",
    "    result['full_lockdown'].to_csv('../Data/stay_home/full_lockdown.csv')\n",
    "    result['release'].to_csv('../Data/stay_home/release.csv')\n",
    "    relative_rate.to_csv('../Data/stay_home/per_date.csv')\n",
    "\n",
    "\n",
    "def create_demograph_sick_pop(ind):\n",
    "    ### Creating demograph/sick_pop.csv\n",
    "    taz2sick = pd.read_csv('../Data/sick/taz2sick.csv')\n",
    "\n",
    "    taz2cell = pd.read_excel(\n",
    "        '../Data/division_choice/' + ind.cell_name + '/taz2cell.xlsx')\n",
    "    pop_dist = pd.read_excel('../Data/raw/pop2taz.xlsx', header=2)\n",
    "    ages_list = ['Unnamed: ' + str(i) for i in range(17, 32)]\n",
    "    pop_dist = pop_dist[['אזור 2630', 'גילאים'] + ages_list]\n",
    "    pop_dist.columns = ['id'] + list(pop_dist.iloc[0, 1:])\n",
    "    pop_dist = pop_dist.drop([0, 2631, 2632, 2633])\n",
    "    pop_dist['tot_pop'] = pop_dist.iloc[:, 1:].sum(axis=1)\n",
    "    pop_dist['tot_pop'].loc[pop_dist['tot_pop'] == 0] = 1\n",
    "    pop_dist = pop_dist.iloc[:, 1:-1].div(pop_dist['tot_pop'], axis=0).join(\n",
    "        pop_dist['id']).join(pop_dist['tot_pop'])\n",
    "    pop_dist['tot_pop'].loc[pop_dist['tot_pop'] == 1] = 0\n",
    "    pop_dist['tot_pop'] = pop_dist['tot_pop'] / pop_dist['tot_pop'].sum()\n",
    "    pop_dist.iloc[:, :-2] = pop_dist.iloc[:, :-2].mul(pop_dist['tot_pop'],\n",
    "                                                      axis=0)\n",
    "    pop_dist = pop_dist[['id', 'tot_pop']]\n",
    "\n",
    "    taz2sick = taz2sick.merge(taz2cell, on='taz_id')\n",
    "    taz2sick = taz2sick.merge(pop_dist, left_on='taz_id', right_on='id')\n",
    "    taz2sick['cell_id'] = taz2sick['cell_id'].astype(str)\n",
    "    empty_cells = pd.read_csv('../Data/demograph/empty_cells.csv')[\n",
    "        'cell_id'].astype(str)\n",
    "    taz2sick = taz2sick[\n",
    "        taz2sick['cell_id'].apply(lambda x: x not in empty_cells.values)]\n",
    "    # taz2sick['cases_prop'] = taz2sick['cases_prop'] * taz2sick['tot_pop']\n",
    "\n",
    "    taz2sick = taz2sick.groupby(by='cell_id')[['cases_prop']].apply(\n",
    "        sum)\n",
    "    taz2sick.name = 'cases_prop'\n",
    "    taz2sick.to_csv('../Data/demograph/sick_prop.csv')\n",
    "\n",
    "\n",
    "def create_stay_idx_routine(ind, start, end, date_beta_behave):\n",
    "    ### Data loading\n",
    "\n",
    "    stay_home_idx = pd.read_csv('../Data/stay_home/per_date.csv',\n",
    "                                      index_col=0)\n",
    "    stay_home_idx.columns = stay_home_idx.columns.astype(str)\n",
    "    stay_home_idx.index = pd.to_datetime(stay_home_idx.index)\n",
    "    stay_home_idx = stay_home_idx[pd.Timestamp(start):]\n",
    "\n",
    "    # preparing model objects:\n",
    "    stay_idx_t = []\n",
    "    routine_vector = []\n",
    "    d_tot = 500\n",
    "\n",
    "    for i in pd.date_range(pd.Timestamp(start), pd.Timestamp(end)):\n",
    "        stay_home_idx_daily = stay_home_idx.loc[i].values\n",
    "        stay_home_idx_daily = expand_partial_array(\n",
    "            mapping_dic=ind.region_gra_dict,\n",
    "            array_to_expand=stay_home_idx_daily,\n",
    "            size=len(ind.GRA),\n",
    "        )\n",
    "        stay_idx_t.append(stay_home_idx_daily)\n",
    "\n",
    "        if i < pd.Timestamp(date_beta_behave):\n",
    "            routine_vector.append(0)\n",
    "        else:\n",
    "            routine_vector.append(1)\n",
    "\n",
    "    stay_home_idx_daily = stay_home_idx.iloc[-1].values\n",
    "    stay_home_idx_daily = expand_partial_array(\n",
    "        mapping_dic=ind.region_gra_dict,\n",
    "        array_to_expand=stay_home_idx_daily,\n",
    "        size=len(ind.GRA),\n",
    "    )\n",
    "    for i in range(d_tot-len(pd.date_range(pd.Timestamp(start), pd.Timestamp(end)))):\n",
    "\n",
    "        stay_idx_t.append(stay_home_idx_daily)\n",
    "        routine_vector.append(1)\n",
    "\n",
    "    stay_idx_calibration = {\n",
    "        'Non-intervention': stay_idx_t,\n",
    "        'Intervention': [0] * 500,\n",
    "    }\n",
    "\n",
    "    routine_vector_calibration = {\n",
    "        'Non-intervention': {\n",
    "            'work': routine_vector,\n",
    "            'not_work': routine_vector\n",
    "        },\n",
    "        'Intervention': {\n",
    "            'work': [1] * 500,\n",
    "            'not_work': [1] * 500,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # save objects\n",
    "    with open('../Data/parameters/stay_home_idx.pickle', 'wb') as handle:\n",
    "        pickle.dump(stay_idx_calibration, handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('../Data/parameters/routine_t.pickle', 'wb') as handle:\n",
    "        pickle.dump(routine_vector_calibration, handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_full_matices(ind):\n",
    "    ### Full Matrixes\n",
    "    with (\n",
    "    open('../Data/division_choice/' + ind.cell_name + '/mat_macro_model_df.pickle',\n",
    "         'rb')) as openfile:\n",
    "        OD_dict = pickle.load(openfile)\n",
    "\n",
    "    base_leisure = pd.read_csv('../Data/raw/leisure_mtx.csv', index_col=0)\n",
    "    base_work = pd.read_csv('../Data/raw/work_mtx.csv', index_col=0)\n",
    "    base_school = pd.read_csv('../Data/raw/school_mtx.csv', index_col=0)\n",
    "\n",
    "    religion_dist = pd.read_csv('../Data/demograph/religion_dis.csv',\n",
    "                                index_col=0)\n",
    "    age_dist_area = pd.read_csv('../Data/demograph/age_dist_area.csv',\n",
    "                                index_col=0)\n",
    "    home_secularism = pd.read_excel('../Data/raw/secularism_base_home.xlsx',\n",
    "                                    index_col=0)\n",
    "    home_haredi = pd.read_excel('../Data/raw/haredi_base_home.xlsx',\n",
    "                                index_col=0)\n",
    "    home_arabs = pd.read_excel('../Data/raw/arabs_base_home.xlsx', index_col=0)\n",
    "\n",
    "    # fix_shahaf_bug\n",
    "    if ind.cell_name == '250':\n",
    "        if len(str(OD_dict[list(OD_dict.keys())[0]].columns[0])) == 6:\n",
    "            print('shahaf bug returned!!!!')\n",
    "            for k in OD_dict.keys():\n",
    "                OD_dict[k].columns = pd.Index(ind.G.values())\n",
    "        if len(str(OD_dict[list(OD_dict.keys())[0]].index[0])) == 6:\n",
    "            for k in OD_dict.keys():\n",
    "                OD_dict[k].index = pd.Index(ind.G.values())\n",
    "\n",
    "    # make sure index of area is string\n",
    "    for k in OD_dict.keys():\n",
    "        OD_dict[k].columns = OD_dict[k].columns.astype(str)\n",
    "        OD_dict[k].index = OD_dict[k].index.astype(str)\n",
    "        OD_dict[k] = OD_dict[k].filter(list(ind.G.values()), axis=1)\n",
    "        OD_dict[k] = OD_dict[k].filter(list(ind.G.values()), axis=0)\n",
    "\n",
    "    ############ 21.2-14.3 #############\n",
    "    full_leisure_routine = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['routine', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 14.3-16.3 #############\n",
    "    full_leisure_no_school = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_school', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 17.3-25.3 #############\n",
    "    full_leisure_no_work = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_work', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 26.3-2.4 #############\n",
    "    full_leisure_no_100_meters = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_100_meters', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 3.4-6.4 #############\n",
    "    full_leisure_no_bb = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_bb', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 7.4-16.4 #############\n",
    "    full_leisure_full_lockdown = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['full_lockdown', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 17.4 - 4.5 #############\n",
    "    full_leisure_release = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['release', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    ############ 5.5 - 11.5 #############\n",
    "    full_leisure_back2routine = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['back2routine', 2],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area\n",
    "    )\n",
    "\n",
    "    # save matrix\n",
    "    try:\n",
    "        os.mkdir('../Data/base_contact_mtx')\n",
    "    except:\n",
    "        pass\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_routine.npz',\n",
    "                          full_leisure_routine)\n",
    "    scipy.sparse.save_npz(\n",
    "        '../Data/base_contact_mtx/full_leisure_no_school.npz',\n",
    "        full_leisure_no_school)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_no_work.npz',\n",
    "                          full_leisure_no_work)\n",
    "    scipy.sparse.save_npz(\n",
    "        '../Data/base_contact_mtx/full_leisure_no_100_meters.npz',\n",
    "        full_leisure_no_100_meters)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_no_bb.npz',\n",
    "                          full_leisure_no_bb)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_full_lockdown.npz',\n",
    "                          full_leisure_full_lockdown)\n",
    "    scipy.sparse.save_npz(\n",
    "        '../Data/base_contact_mtx/full_leisure_release.npz',\n",
    "        full_leisure_release)\n",
    "    scipy.sparse.save_npz(\n",
    "        '../Data/base_contact_mtx/full_leisure_back2routine.npz',\n",
    "        full_leisure_back2routine)\n",
    "\n",
    "    # creating school- work matrix;\n",
    "    base_work_school = base_work.copy()\n",
    "    base_work_school.loc['0-4'] = base_school.loc['0-4']\n",
    "    base_work_school.loc['5-9'] = base_school.loc['5-9']\n",
    "    base_work_school['0-4'] = base_school['0-4']\n",
    "    base_work_school['5-9'] = base_school['5-9']\n",
    "    # creating eye matrix\n",
    "    eye_OD = OD_dict['routine', 1].copy()\n",
    "\n",
    "    for col in eye_OD.columns:\n",
    "        eye_OD[col].values[:] = 0\n",
    "    eye_OD.values[tuple([np.arange(eye_OD.shape[0])] * 2)] = 1\n",
    "\n",
    "    ############ 21.2-14.3 #############\n",
    "    full_work_routine = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['routine', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 14.3-16.3 #############\n",
    "    full_work_no_school = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_school', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 17.3-25.3 #############\n",
    "    full_work_no_work = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_work', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 26.3-2.4 #############\n",
    "    full_work_no_100_meters = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_100_meters', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 3.4-6.4 #############\n",
    "    full_work_no_bb = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['no_bb', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 7.4-16.4 #############\n",
    "    full_work_full_lockdown = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['full_lockdown', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 17.4 - 4.5 #############\n",
    "    full_work_release = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['release', 1],\n",
    "        base_mat=base_work_school,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    ############ 5.5 - 11.5 #############\n",
    "    full_work_back2routine = create_C_mtx_leisure_work(\n",
    "        ind=ind,\n",
    "        od_mat=OD_dict['back2routine', 1],\n",
    "        base_mat=base_leisure,\n",
    "        age_dist_area=age_dist_area,\n",
    "        eye_mat=eye_OD,\n",
    "    )\n",
    "\n",
    "    # save matrix\n",
    "    try:\n",
    "        os.mkdir('../Data/base_contact_mtx')\n",
    "    except:\n",
    "        pass\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_routine.npz',\n",
    "                          full_work_routine)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_no_school.npz',\n",
    "                          full_work_no_school)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_no_work.npz',\n",
    "                          full_work_no_work)\n",
    "    scipy.sparse.save_npz(\n",
    "        '../Data/base_contact_mtx/full_work_no_100_meters.npz',\n",
    "        full_work_no_100_meters)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_no_bb.npz',\n",
    "                          full_work_no_bb)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_full_lockdown.npz',\n",
    "                          full_work_full_lockdown)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_release.npz',\n",
    "                          full_work_release)\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_back2routine.npz',\n",
    "                          full_work_back2routine)\n",
    "\n",
    "    ## Home Matices\n",
    "    full_home = pd.DataFrame(\n",
    "        index=pd.MultiIndex.from_tuples(list(ind.MI.values()),\n",
    "                                        names=['age', 'area', 'age']),\n",
    "        columns=OD_dict['routine', 0].index)\n",
    "\n",
    "    religion_dist.set_index('cell_id', inplace=True)\n",
    "    religion_dist.index = religion_dist.index.astype(str)\n",
    "\n",
    "    # fill the matrix:\n",
    "    for index in list(full_home.index):\n",
    "        religion_area = religion_dist.loc[index[1]].copy()\n",
    "        cell_val = religion_area['Orthodox'] * home_haredi.loc[index[0]][\n",
    "            index[2]] + \\\n",
    "                   religion_area['Sacular'] * home_secularism.loc[index[0]][\n",
    "                       index[2]] + \\\n",
    "                   religion_area['Christian'] * home_arabs.loc[index[0]][\n",
    "                       index[2]] + \\\n",
    "                   religion_area['Other'] * home_secularism.loc[index[0]][\n",
    "                       index[2]] + \\\n",
    "                   religion_area['Druze'] * home_arabs.loc[index[0]][\n",
    "                       index[2]] + \\\n",
    "                   religion_area['Muslim'] * home_arabs.loc[index[0]][index[2]]\n",
    "        full_home.loc[index] = (eye_OD.loc[index[1]] * cell_val) / \\\n",
    "                               age_dist_area[index[2]]\n",
    "\n",
    "    full_home = csr_matrix(full_home.unstack().reorder_levels(\n",
    "        ['area', 'age']).sort_index().values.astype(float))\n",
    "    # save matrix\n",
    "    try:\n",
    "        os.mkdir('../Data/base_contact_mtx')\n",
    "    except:\n",
    "        pass\n",
    "    scipy.sparse.save_npz('../Data/base_contact_mtx/full_home.npz', full_home)\n",
    "\n",
    "\n",
    "def create_parameters_indices(ind):\n",
    "    with open('../Data/parameters/indices.pickle', 'wb') as handle:\n",
    "        pickle.dump(ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_parameters_f0(ind):\n",
    "    ### Asymptomatic\n",
    "    asymp = pd.read_csv('../Data/raw/asymptomatic_proportions.csv',\n",
    "                        index_col=0)\n",
    "    f0_full = {}  # dict that contains the possible scenarios\n",
    "\n",
    "    # asymptomatic with risk group, high risk with 0\n",
    "    f_init = np.zeros(len(list(itertools.product(ind.R.values(), ind.A.values()))))\n",
    "    for i in [1, 2, 3]:\n",
    "        f_tmp = f_init.copy()\n",
    "        f_tmp[:8] = asymp['Scenario ' + str(i)].values[:-1]\n",
    "        f_tmp[9:] = asymp['Scenario ' + str(i)].values[:-1]\n",
    "        f0_full['Scenario' + str(i)] = expand_partial_array(ind.risk_age_dict,\n",
    "                                                            f_tmp, len(ind.N))\n",
    "    # Save\n",
    "    try:\n",
    "        os.mkdir('../Data/parameters')\n",
    "    except:\n",
    "        pass\n",
    "    with open('../Data/parameters/f0_full.pickle', 'wb') as handle:\n",
    "        pickle.dump(f0_full, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_parameters_eps_by_region_prop(ind, age_dist):\n",
    "    asymp = pd.read_csv('../Data/raw/asymptomatic_proportions.csv',\n",
    "                        index_col=0)\n",
    "    ### eps by region proportion\n",
    "    risk_dist = pd.read_csv('../Data/raw/population_size.csv')\n",
    "    init_I_dis_italy = pd.read_csv('../Data/raw/init_i_italy.csv')[\n",
    "                           'proportion'].values[:-1]\n",
    "    f_init = pd.read_pickle('../Data/parameters/f0_full.pickle')\n",
    "    eps_t = {}\n",
    "    init_I_IL = {}\n",
    "    init_I_dis = {}\n",
    "    for i in [1, 2, 3]:\n",
    "        scen = 'Scenario' + str(i)\n",
    "        f_init_i = f_init[scen][:(len(ind.R) * len(ind.A))]\n",
    "        init_I_IL[scen] = (491. / (1 - asymp['Scenario ' + str(i)].values[-1])) / isr_pop\n",
    "        init_I_dis[scen] = init_I_dis_italy * init_I_IL[scen]\n",
    "\n",
    "\n",
    "    # Loading data\n",
    "    region_prop = pd.read_csv('../Data/demograph/sick_prop.csv', index_col=0)[\n",
    "        'cases_prop'].copy()\n",
    "    region_prop.index = region_prop.index.astype(str)\n",
    "    risk_prop = pd.read_csv('../Data/raw/risk_dist.csv', index_col=0)[\n",
    "        'risk'].copy()\n",
    "    eps_t_region = {}\n",
    "    for sc, init_I in zip(init_I_dis.keys(), init_I_dis.values()):\n",
    "        eps_temp = []\n",
    "        for t in range(1000):\n",
    "            if t < len(init_I):\n",
    "                # empty array for day t\n",
    "                day_vec = np.zeros(len(ind.N))\n",
    "                # fill in the array, zero for intervention groups\n",
    "                for key in ind.N.keys():\n",
    "                    if ind.N[key][0] == 'Intervention':\n",
    "                        day_vec[key] = 0\n",
    "                    else:\n",
    "                        day_vec[key] = init_I[t] * region_prop[ind.N[key][1]] * \\\n",
    "                                       age_dist[ind.N[key][3]] * \\\n",
    "                                       (risk_prop[ind.N[key][3]] ** (\n",
    "                                               1 - (ind.N[key][2] == 'Low'))) * \\\n",
    "                                       ((1 - risk_prop[ind.N[key][3]]) ** (\n",
    "                                               ind.N[key][2] == 'Low'))\n",
    "                eps_temp.append(day_vec)\n",
    "            else:\n",
    "                eps_temp.append(0.0)\n",
    "\n",
    "            eps_t_region[sc] = eps_temp\n",
    "    # save eps:\n",
    "    with open('../Data/parameters/eps_by_region.pickle', 'wb') as handle:\n",
    "        pickle.dump(eps_t_region, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_parameters_hosptialization(ind):\n",
    "    ### hospitalization\n",
    "    hosp_init = pd.read_csv('../Data/raw/hospitalizations.csv')\n",
    "    hosp = expand_partial_array(ind.risk_age_dict, hosp_init['pr_hosp'].values,\n",
    "                                len(ind.N))\n",
    "    # Save\n",
    "    with open('../Data/parameters/hospitalization.pickle', 'wb') as handle:\n",
    "        pickle.dump(hosp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_parameters_vents_proba(ind):\n",
    "    ### Ventilation\n",
    "    vents_init = pd.read_csv('../Data/raw/vent_proba.csv')\n",
    "    vent = expand_partial_array(ind.risk_age_dict, vents_init['pr_vents'].values,\n",
    "                                len(ind.N))\n",
    "    # Save\n",
    "    with open('../Data/parameters/vents_proba.pickle', 'wb') as handle:\n",
    "        pickle.dump(vent, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_parameters_C_calibration(ind):\n",
    "    ### Calibration contact matrix\n",
    "    full_mtx_home = scipy.sparse.load_npz(\n",
    "        '../Data/base_contact_mtx/full_home.npz')\n",
    "\n",
    "    full_mtx_work = {\n",
    "        'routine': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_routine.npz'),\n",
    "        'no_school': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_no_school.npz'),\n",
    "        'no_work': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_no_work.npz'),\n",
    "        'no_100_meters': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_no_100_meters.npz'),\n",
    "        'no_bb': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_no_bb.npz'),\n",
    "        'full_lockdown': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_full_lockdown.npz'),\n",
    "        'release': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_release.npz'),\n",
    "        'back2routine': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_work_back2routine.npz'),\n",
    "    }\n",
    "\n",
    "    full_mtx_leisure = {\n",
    "        'routine': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_routine.npz'),\n",
    "        'no_school': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_no_school.npz'),\n",
    "        'no_work': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_no_work.npz'),\n",
    "        'no_100_meters': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_no_100_meters.npz'),\n",
    "        'no_bb': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_no_bb.npz'),\n",
    "        'full_lockdown': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_full_lockdown.npz'),\n",
    "        'release': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_release.npz'),\n",
    "        'back2routine': scipy.sparse.load_npz(\n",
    "            '../Data/base_contact_mtx/full_leisure_back2routine.npz'),\n",
    "    }\n",
    "    C_calibration = {}\n",
    "    d_tot = 500\n",
    "    # no intervation are null groups\n",
    "    home_inter = []\n",
    "    work_inter = []\n",
    "    leis_inter = []\n",
    "\n",
    "    for i in range(d_tot):\n",
    "        home_inter.append(\n",
    "            csr_matrix((full_mtx_home.shape[0], full_mtx_home.shape[1])))\n",
    "        work_inter.append(csr_matrix((full_mtx_work['routine'].shape[0],\n",
    "                                         full_mtx_work['routine'].shape[1])))\n",
    "        leis_inter.append(csr_matrix((full_mtx_leisure['routine'].shape[0],\n",
    "                                         full_mtx_leisure['routine'].shape[\n",
    "                                             1])))\n",
    "\n",
    "    # Intervantion\n",
    "    home_no_inter = []\n",
    "    work_no_inter = []\n",
    "    leis_no_inter = []\n",
    "\n",
    "    # first days of routine from Feb 21st - March 13th\n",
    "    d_rout = 9 + 13\n",
    "    for i in range(d_rout):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['routine'])\n",
    "        leis_no_inter.append(full_mtx_leisure['routine'])\n",
    "\n",
    "    # first days of no school from March 14th - March 16th\n",
    "    d_no_school = 3\n",
    "    for i in range(d_no_school):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['no_school'])\n",
    "        leis_no_inter.append(full_mtx_leisure['no_school'])\n",
    "\n",
    "    # without school and work from March 17th - March 25th\n",
    "    d_no_work = 9\n",
    "    for i in range(d_no_work):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['no_work'])\n",
    "        leis_no_inter.append(full_mtx_leisure['no_work'])\n",
    "\n",
    "    # 100 meters constrain from March 26th - April 2nd\n",
    "    d_no_100_meters = 8\n",
    "    for i in range(d_no_100_meters):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['no_100_meters'])\n",
    "        leis_no_inter.append(full_mtx_leisure['no_100_meters'])\n",
    "\n",
    "    # Bnei Brak quaranrine from April 3rd - April 18th\n",
    "    d_bb = 16\n",
    "    for i in range(d_bb):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['no_bb'])\n",
    "        leis_no_inter.append(full_mtx_leisure['no_bb'])\n",
    "\n",
    "\n",
    "    # full lockdown from April 7th - April 16th\n",
    "    d_full_lockdown = 10\n",
    "    for i in range(d_full_lockdown):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['full_lockdown'])\n",
    "        leis_no_inter.append(full_mtx_leisure['full_lockdown'])\n",
    "\n",
    "    # full release from April 17th - May 4th\n",
    "    d_release = 18\n",
    "    for i in range(d_release):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['release'])\n",
    "        leis_no_inter.append(full_mtx_leisure['release'])\n",
    "\n",
    "    # full release from April 5th - May 11th\n",
    "    for i in range(d_tot - (\n",
    "            d_rout + d_no_school + d_no_work + d_no_100_meters + d_bb\n",
    "            + d_full_lockdown + d_release)):\n",
    "        home_no_inter.append(full_mtx_home)\n",
    "        work_no_inter.append(full_mtx_work['back2routine'])\n",
    "        leis_no_inter.append(full_mtx_leisure['back2routine'])\n",
    "\n",
    "    C_calibration['home_inter'] = home_inter\n",
    "    C_calibration['work_inter'] = work_inter\n",
    "    C_calibration['leisure_inter'] = leis_inter\n",
    "    C_calibration['home_non'] = home_no_inter\n",
    "    C_calibration['work_non'] = work_no_inter\n",
    "    C_calibration['leisure_non'] = leis_no_inter\n",
    "\n",
    "    # Save\n",
    "    with open('../Data/parameters/C_calibration.pickle', 'wb') as handle:\n",
    "        pickle.dump(C_calibration, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_parameters_is_haredim(ind):\n",
    "    ### Haredim vector\n",
    "    hared_dis = pd.read_csv('../Data/demograph/religion_dis.csv', index_col=0)[\n",
    "        ['cell_id', 'Orthodox']].copy()\n",
    "    hared_dis.set_index('cell_id', inplace=True)\n",
    "    hared_dis.index = hared_dis.index.astype(str)\n",
    "    # Creating model orthodox dist. and save it as pickle\n",
    "    model_orthodox_dis = np.zeros(len(ind.GA))\n",
    "    for i in ind.GA.keys():\n",
    "        model_orthodox_dis[i] = hared_dis.loc[str(ind.GA[i][0])]\n",
    "    with open('../Data/parameters/orthodox_dist.pickle', 'wb') as handle:\n",
    "        pickle.dump(model_orthodox_dis, handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def create_parameters_is_arab(ind):\n",
    "    ### Arabs vector\n",
    "    arab_dis = pd.read_csv('../Data/demograph/religion_dis.csv', index_col=0)[\n",
    "        ['cell_id', 'Druze', 'Muslim', 'Christian']].copy()\n",
    "    arab_dis.set_index('cell_id', inplace=True)\n",
    "    arab_dis.index = arab_dis.index.astype(str)\n",
    "    arab_dis = arab_dis.sum(axis=1)\n",
    "    # Creating model arab dist. and save it as pickle\n",
    "    model_arab_dis = np.zeros(len(ind.GA))\n",
    "    for i in ind.GA.keys():\n",
    "        model_arab_dis[i] = arab_dis.loc[str(ind.GA[i][0])]\n",
    "    with open('../Data/parameters/arab_dist.pickle', 'wb') as handle:\n",
    "        pickle.dump(model_arab_dis, handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def create_init_pop(ind):\n",
    "    age_dist_area = pd.read_csv('../Data/demograph/age_dist_area.csv')\n",
    "    age_dist_area.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    age_dist_area.set_index('cell_id', inplace=True)\n",
    "    age_dist_area = age_dist_area.stack()\n",
    "    init_pop = expand_partial_array(ind.region_age_dict, age_dist_area.values,\n",
    "                                    len(ind.N))\n",
    "    init_pop[ind.inter_dict['Intervention']] = 0\n",
    "    risk_pop = pd.read_csv('../Data/raw/risk_dist.csv')\n",
    "    risk_pop.set_index('Age', inplace=True)\n",
    "    risk_pop['High'] = risk_pop['risk']\n",
    "    risk_pop['Low'] = 1 - risk_pop['risk']\n",
    "    risk_pop.drop(['risk'], axis=1, inplace=True)\n",
    "    risk_pop = risk_pop.stack()\n",
    "    risk_pop.index = risk_pop.index.swaplevel(0, 1)\n",
    "    risk_pop = risk_pop.unstack().stack()\n",
    "\n",
    "    for (r, a), g_idx in zip(ind.risk_age_dict.keys(),\n",
    "                             ind.risk_age_dict.values()):\n",
    "        init_pop[g_idx] = init_pop[g_idx] * risk_pop[r, a]\n",
    "\n",
    "    # Age distribution:\n",
    "    pop_dist = init_pop\n",
    "    # Save\n",
    "    with open('../Data/parameters/init_pop.pickle', 'wb') as handle:\n",
    "        pickle.dump(pop_dist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "### define indices\n",
    "# ind = Indices(cell_name)\n",
    "\n",
    "# age_dist = {'0-4': 0.02, '5-9': 0.02, '10-19': 0.11, '20-29': 0.23,\n",
    "#             '30-39': 0.15, '40-49': 0.14, '50-59': 0.14, '60-69': 0.11,\n",
    "#             '70+': 0.08}\n",
    "\n",
    "# create_demograph_age_dist_empty_cells(ind)\n",
    "\n",
    "# ind = create_paramaters_ind(ind)\n",
    "\n",
    "# create_demograph_religion(ind)\n",
    "\n",
    "# create_stay_home(ind)\n",
    "\n",
    "# create_demograph_sick_pop(ind)\n",
    "\n",
    "# create_stay_idx_routine(ind, '2020-02-20', '2020-05-08', '2020-03-14')\n",
    "\n",
    "# create_full_matices(ind)\n",
    "\n",
    "# create_parameters_indices(ind)\n",
    "\n",
    "# create_parameters_f0(ind)\n",
    "\n",
    "# create_parameters_eps_by_region_prop(ind, age_dist)\n",
    "\n",
    "# create_parameters_hosptialization(ind)\n",
    "\n",
    "# create_parameters_vents_proba(ind)\n",
    "\n",
    "# create_parameters_C_calibration(ind)\n",
    "\n",
    "# create_parameters_is_haredim(ind)\n",
    "\n",
    "create_parameters_is_arab(ind)\n",
    "\n",
    "create_init_pop(ind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open('../Data/parameters/indices.pickle', 'rb')) as openfile:\n",
    "    ind = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Full Matrixes\n",
    "with (\n",
    "open('../Data/division_choice/' + ind.cell_name + '/mat_macro_model_df.pickle',\n",
    "     'rb')) as openfile:\n",
    "    OD_dict = pickle.load(openfile)\n",
    "\n",
    "base_leisure = pd.read_csv('../Data/raw/leisure_mtx.csv', index_col=0)\n",
    "base_work = pd.read_csv('../Data/raw/work_mtx.csv', index_col=0)\n",
    "base_school = pd.read_csv('../Data/raw/school_mtx.csv', index_col=0)\n",
    "\n",
    "religion_dist = pd.read_csv('../Data/demograph/religion_dis.csv',\n",
    "                            index_col=0)\n",
    "age_dist_area = pd.read_csv('../Data/demograph/age_dist_area.csv',\n",
    "                            index_col=0)\n",
    "home_secularism = pd.read_excel('../Data/raw/secularism_base_home.xlsx',\n",
    "                                index_col=0)\n",
    "home_haredi = pd.read_excel('../Data/raw/haredi_base_home.xlsx',\n",
    "                            index_col=0)\n",
    "home_arabs = pd.read_excel('../Data/raw/arabs_base_home.xlsx', index_col=0)\n",
    "\n",
    "# fix_shahaf_bug\n",
    "if ind.cell_name == '250':\n",
    "    if len(str(OD_dict[list(OD_dict.keys())[0]].columns[0])) == 6:\n",
    "        print('shahaf bug returned!!!!')\n",
    "        for k in OD_dict.keys():\n",
    "            OD_dict[k].columns = pd.Index(ind.G.values())\n",
    "    if len(str(OD_dict[list(OD_dict.keys())[0]].index[0])) == 6:\n",
    "        for k in OD_dict.keys():\n",
    "            OD_dict[k].index = pd.Index(ind.G.values())\n",
    "\n",
    "# make sure index of area is string\n",
    "for k in OD_dict.keys():\n",
    "    OD_dict[k].columns = OD_dict[k].columns.astype(str)\n",
    "    OD_dict[k].index = OD_dict[k].index.astype(str)\n",
    "    OD_dict[k] = OD_dict[k].filter(list(ind.G.values()), axis=1)\n",
    "    OD_dict[k] = OD_dict[k].filter(list(ind.G.values()), axis=0)\n",
    "\n",
    "# ############ 21.2-14.3 #############\n",
    "# full_leisure_routine = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['routine', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 14.3-16.3 #############\n",
    "# full_leisure_no_school = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_school', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 17.3-25.3 #############\n",
    "# full_leisure_no_work = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_work', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 26.3-2.4 #############\n",
    "# full_leisure_no_100_meters = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_100_meters', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 3.4-6.4 #############\n",
    "# full_leisure_no_bb = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_bb', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 7.4-16.4 #############\n",
    "# full_leisure_full_lockdown = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['full_lockdown', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 17.4 - 4.5 #############\n",
    "# full_leisure_release = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['release', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# ############ 5.5 - 11.5 #############\n",
    "# full_leisure_back2routine = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['back2routine', 2],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area\n",
    "# )\n",
    "\n",
    "# # save matrix\n",
    "# try:\n",
    "#     os.mkdir('../Data/base_contact_mtx')\n",
    "# except:\n",
    "#     pass\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_routine.npz',\n",
    "#                       full_leisure_routine)\n",
    "# scipy.sparse.save_npz(\n",
    "#     '../Data/base_contact_mtx/full_leisure_no_school.npz',\n",
    "#     full_leisure_no_school)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_no_work.npz',\n",
    "#                       full_leisure_no_work)\n",
    "# scipy.sparse.save_npz(\n",
    "#     '../Data/base_contact_mtx/full_leisure_no_100_meters.npz',\n",
    "#     full_leisure_no_100_meters)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_no_bb.npz',\n",
    "#                       full_leisure_no_bb)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_leisure_full_lockdown.npz',\n",
    "#                       full_leisure_full_lockdown)\n",
    "# scipy.sparse.save_npz(\n",
    "#     '../Data/base_contact_mtx/full_leisure_release.npz',\n",
    "#     full_leisure_release)\n",
    "# scipy.sparse.save_npz(\n",
    "#     '../Data/base_contact_mtx/full_leisure_back2routine.npz',\n",
    "#     full_leisure_back2routine)\n",
    "\n",
    "# # creating school- work matrix;\n",
    "# base_work_school = base_work.copy()\n",
    "# base_work_school.loc['0-4'] = base_school.loc['0-4']\n",
    "# base_work_school.loc['5-9'] = base_school.loc['5-9']\n",
    "# base_work_school['0-4'] = base_school['0-4']\n",
    "# base_work_school['5-9'] = base_school['5-9']\n",
    "# # creating eye matrix\n",
    "# eye_OD = OD_dict['routine', 1].copy()\n",
    "\n",
    "# for col in eye_OD.columns:\n",
    "#     eye_OD[col].values[:] = 0\n",
    "# eye_OD.values[tuple([np.arange(eye_OD.shape[0])] * 2)] = 1\n",
    "\n",
    "# ############ 21.2-14.3 #############\n",
    "# full_work_routine = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['routine', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 14.3-16.3 #############\n",
    "# full_work_no_school = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_school', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 17.3-25.3 #############\n",
    "# full_work_no_work = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_work', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 26.3-2.4 #############\n",
    "# full_work_no_100_meters = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_100_meters', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 3.4-6.4 #############\n",
    "# full_work_no_bb = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['no_bb', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 7.4-16.4 #############\n",
    "# full_work_full_lockdown = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['full_lockdown', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 17.4 - 4.5 #############\n",
    "# full_work_release = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['release', 1],\n",
    "#     base_mat=base_work_school,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# ############ 5.5 - 11.5 #############\n",
    "# full_work_back2routine = create_C_mtx_leisure_work(\n",
    "#     ind=ind,\n",
    "#     od_mat=OD_dict['back2routine', 1],\n",
    "#     base_mat=base_leisure,\n",
    "#     age_dist_area=age_dist_area,\n",
    "#     eye_mat=eye_OD,\n",
    "# )\n",
    "\n",
    "# # save matrix\n",
    "# try:\n",
    "#     os.mkdir('../Data/base_contact_mtx')\n",
    "# except:\n",
    "#     pass\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_routine.npz',\n",
    "#                       full_work_routine)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_no_school.npz',\n",
    "#                       full_work_no_school)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_no_work.npz',\n",
    "#                       full_work_no_work)\n",
    "# scipy.sparse.save_npz(\n",
    "#     '../Data/base_contact_mtx/full_work_no_100_meters.npz',\n",
    "#     full_work_no_100_meters)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_no_bb.npz',\n",
    "#                       full_work_no_bb)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_full_lockdown.npz',\n",
    "#                       full_work_full_lockdown)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_release.npz',\n",
    "#                       full_work_release)\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_work_back2routine.npz',\n",
    "#                       full_work_back2routine)\n",
    "\n",
    "# ## Home Matices\n",
    "# full_home = pd.DataFrame(\n",
    "#     index=pd.MultiIndex.from_tuples(list(ind.MI.values()),\n",
    "#                                     names=['age', 'area', 'age']),\n",
    "#     columns=OD_dict['routine', 0].index)\n",
    "\n",
    "# religion_dist.set_index('cell_id', inplace=True)\n",
    "# religion_dist.index = religion_dist.index.astype(str)\n",
    "\n",
    "# # fill the matrix:\n",
    "# for index in list(full_home.index):\n",
    "#     religion_area = religion_dist.loc[index[1]].copy()\n",
    "#     cell_val = religion_area['Orthodox'] * home_haredi.loc[index[0]][\n",
    "#         index[2]] + \\\n",
    "#                religion_area['Sacular'] * home_secularism.loc[index[0]][\n",
    "#                    index[2]] + \\\n",
    "#                religion_area['Christian'] * home_arabs.loc[index[0]][\n",
    "#                    index[2]] + \\\n",
    "#                religion_area['Other'] * home_secularism.loc[index[0]][\n",
    "#                    index[2]] + \\\n",
    "#                religion_area['Druze'] * home_arabs.loc[index[0]][\n",
    "#                    index[2]] + \\\n",
    "#                religion_area['Muslim'] * home_arabs.loc[index[0]][index[2]]\n",
    "#     full_home.loc[index] = (eye_OD.loc[index[1]] * cell_val) / \\\n",
    "#                            age_dist_area[index[2]]\n",
    "\n",
    "# full_home = csr_matrix(full_home.unstack().reorder_levels(\n",
    "#     ['area', 'age']).sort_index().values.astype(float))\n",
    "# # save matrix\n",
    "# try:\n",
    "#     os.mkdir('../Data/base_contact_mtx')\n",
    "# except:\n",
    "#     pass\n",
    "# scipy.sparse.save_npz('../Data/base_contact_mtx/full_home.npz', full_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_dict['no_mobility', 1] = OD_dict['routine', 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_dict['no_mobility', 1].loc[:,:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000001</th>\n",
       "      <th>2000002</th>\n",
       "      <th>2000003</th>\n",
       "      <th>2000004</th>\n",
       "      <th>2000005</th>\n",
       "      <th>2000006</th>\n",
       "      <th>2000007</th>\n",
       "      <th>2000008</th>\n",
       "      <th>2000009</th>\n",
       "      <th>2000010</th>\n",
       "      <th>...</th>\n",
       "      <th>2000241</th>\n",
       "      <th>2000242</th>\n",
       "      <th>2000243</th>\n",
       "      <th>2000244</th>\n",
       "      <th>2000245</th>\n",
       "      <th>2000246</th>\n",
       "      <th>2000247</th>\n",
       "      <th>2000248</th>\n",
       "      <th>2000249</th>\n",
       "      <th>2000250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000001</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000002</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000003</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000004</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000005</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000246</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000247</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000248</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000249</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000250</th>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2000001   2000002   2000003   2000004   2000005   2000006   2000007  \\\n",
       "2000001  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000002  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000003  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000004  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000005  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "2000246  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000247  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000248  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000249  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "2000250  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149   \n",
       "\n",
       "          2000008   2000009   2000010  ...   2000241   2000242   2000243  \\\n",
       "2000001  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000002  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000003  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000004  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000005  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "2000246  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000247  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000248  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000249  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "2000250  0.004149  0.004149  0.004149  ...  0.004149  0.004149  0.004149   \n",
       "\n",
       "          2000244   2000245   2000246   2000247   2000248   2000249   2000250  \n",
       "2000001  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000002  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000003  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000004  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000005  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "2000246  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000247  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000248  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000249  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "2000250  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  0.004149  \n",
       "\n",
       "[241 rows x 241 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OD_dict['no_mobility', 1].apply(lambda row: row / row[list(ind.G.values())].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
